{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rpvyrrslgx5f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install faiss-gpu\n",
    "!pip install langchain\n",
    "!pip install langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZLmKneIDOp2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --no-build-isolation --force-reinstall \\\n",
    "    \"boto3>=1.28.57\" \\\n",
    "    \"awscli>=1.29.57\" \\\n",
    "    \"botocore>=1.31.57\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6G_Cwpccyc7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from botocore.client import Config\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CejcJyiphNa9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# will be using Titan Embedding model called form LangChain to generate embeddings of querry\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "# for Data Ingestion\n",
    "import numpy as np\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# For Vectore embedding and Vectore Store (Using Fiass DB fot embedding)\n",
    "from langchain.vectorstores import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "## Prompt eng\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpW8uR1hfjCZ"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"hugginglearners/netflix-shows\")\n",
    "dataset = pd.DataFrame(dataset['train'])\n",
    "# dataset.to_json('/content/drive/MyDrive/Netflix_Project/data.json')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-h9dMSwHjZ_I"
   },
   "outputs": [],
   "source": [
    "# Upload the file to an S3 bucket.\n",
    "session = boto3.Session()\n",
    "s3 = session.resource('s3')\n",
    "bucket = s3.Bucket('moviedatabucket')\n",
    "# bucket.Object('dataset.json').put(Body=open('dataset.json', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5zI_Fb6mqPq"
   },
   "outputs": [],
   "source": [
    "# Bedrock Client\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='AWS_ACCESS_KEY',\n",
    "    aws_secret_access_key='AWS_SECRET_ACCESS_KEY',\n",
    "    aws_session_token=boto3.client('sts').assume_role(\n",
    "        RoleArn='arn:aws:iam::975050062872:role/ColabAccess',\n",
    "        RoleSessionName='ColabSession'  # You can customize the session name\n",
    "    )['Credentials']['SessionToken'],\n",
    ")\n",
    "region =session.region_name\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cxpu7Hbbmzeg"
   },
   "outputs": [],
   "source": [
    "bedrock_config = Config(connect_timeout=120, read_timeout=120, retries={'max_attempts': 3})\n",
    "\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name = region)\n",
    "\n",
    "# bedrock_client.create_evaluation_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iuiDhvcbCYkL"
   },
   "outputs": [],
   "source": [
    "# Instance for Titan Embedding model from bedrock\n",
    "bedrock_embedding = BedrockEmbeddings(model_id='amazon.titan-embed-text-v2:0',\n",
    "                                      client = bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sn6CIxkF7v3"
   },
   "outputs": [],
   "source": [
    "# Implementing data Ingestion\n",
    "obj = bucket.Object('data.json')\n",
    "response = obj.get()\n",
    "data = json.load(response['Body'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_ingest():\n",
    "    loader = TextLoader('data.json', encoding='utf8')\n",
    "    data = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size  = 10000,\n",
    "                                                   chunk_overlap = 500)\n",
    "    doc_chunk =  text_splitter.split_documents(data)\n",
    "    return doc_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyAb1hqAIJz3"
   },
   "outputs": [],
   "source": [
    "# Vector Embedding and Vector store\n",
    "def get_vectorestore(docs):\n",
    "    vectore_store_fiass = FAISS.from_documents(docs,\n",
    "                                               bedrock_embedding)\n",
    "    vectore_store_fiass.save_local('faiss_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NvF8E2jLXzD"
   },
   "outputs": [],
   "source": [
    "# Create/call LLM\n",
    "\n",
    "def get_llama():\n",
    "  # Create Anthropic Model\n",
    "    llm = Bedrock(model_id='meta.llama3-2-1b-instruct-v1:0',\n",
    "                client=bedrock_client,\n",
    "                model_kwargs={'max_gen_len': 300,'temperature': 0.3})\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHGF9ETsOFeS"
   },
   "outputs": [],
   "source": [
    "# Create Prompt template\n",
    "\n",
    "prompt_temp = \"\"\"\n",
    "Human: Answer the question asked at the end, use the context given to you, if you dont have the answer say dont know, do not makeup some answer.\n",
    "Keep the answer precies and brife don't make long lenthy answers. If asked for suggestion give top 3 suggestion saying you can suggest more if user want.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {Question}\n",
    "Assistant:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_temp, input_variables=['context', 'Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2Jj4WlKP710"
   },
   "outputs": [],
   "source": [
    "# Get response from LLM\n",
    "\n",
    "def get_response_llm(llm, vectorestore_fiass, query):\n",
    "    # Get Retrival object using AmazonRetrival from Langchain framework\n",
    "    retriever = AmazonKnowledgeBasesRetriever(\n",
    "          knowledge_base_id= vectorestore_fiass,\n",
    "          retrieval_config={\"vectorSearchConfiguration\":\n",
    "                            {\"numberOfResults\": 4,\n",
    "                            'overrideSearchType': \"SEMANTIC\", # optional\n",
    "                            }\n",
    "                            },)\n",
    "\n",
    "    # Get the context\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "      llm=llm,\n",
    "      chain_type='stuff',\n",
    "      retriever=retriever,\n",
    "      return_source_documents=True,\n",
    "      chain_type_kwargs={'prompt':prompt}\n",
    "    )\n",
    "    answer = qa({'query': query})\n",
    "    return answer['result']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o17r8FkMUZre"
   },
   "outputs": [],
   "source": [
    "doc_chunks = data_ingest()\n",
    "print(type(data_ingest()))\n",
    "get_vectorestore(doc_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rd0WI5oaGR0T"
   },
   "outputs": [],
   "source": [
    "# prompt: save a vectore on drive at given path\n",
    "file_name = \"vectore_store.pkl\"\n",
    "\n",
    "# Upload file to S3\n",
    "bucket.Object(file_name).put(Body=open(file_name, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LLM\n",
    "llm = get_llama()\n",
    "\n",
    "# Ask a query\n",
    "query = input(\"Ask your movie-related question: \")\n",
    "\n",
    "# Get response\n",
    "response = get_response_llm(llm, \"faiss_index\", query)  # if faiss_index is the path\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_EkK2Ki-cCst"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
